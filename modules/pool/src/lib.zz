using <string.h>::{memset};
using <stdio.h>::{stderr, printf};
using hex;
using err;
using <stdint.h>::{uintptr_t};

inline using "asan.h"::{
    ASAN_POISON_MEMORY_REGION,
    ASAN_UNPOISON_MEMORY_REGION
}

const usize ALIGN = (usize)sizeof(uintptr_t);

/// memory m was allocated from pool p
pub theory member(void * m, Pool*p) -> bool;

/// all allocations from pool p are exacly blocksize long,
/// the pool has no multi-block allocations and can be used with each() 
pub theory continuous(Pool p) -> bool;

export struct Pool+ {
    usize   blocksize;
    usize   poolsize;

    u8 mut* used;
    u8 mut* pool;


    u8  mem[];
}

/// creates a new pool with blocksize
///
/// new+1000 mypool = pool::make(10);
/// mypool.alloc(); // get a single block of 10 bytes
/// mypool.malloc(22); // get a continuous memory span of 22 bytes (less efficient)
///
export fn make(Pool+pt mut new*self, usize mut blocksize)
    model continuous(*self)
{
    // 8 byte redzone
    blocksize += 8;
    // 8 byte align
    if blocksize % ALIGN > 0 {
        blocksize += (ALIGN - (blocksize % ALIGN));
    }


    err::assert(pt > pt/(usize)blocksize);
    err::assert((usize)blocksize % ALIGN == 0);

    self->blocksize = blocksize;
    usize mut usedmemlen = pt / (usize)blocksize / 8;
    unsafe { usedmemlen += (ALIGN - (usedmemlen % ALIGN)); }

    self->used  = self->mem;
    self->pool  = self->mem + usedmemlen;

    err::assert((usize)self->used % ALIGN == 0);
    err::assert((usize)self->pool % ALIGN == 0);

    self->poolsize = pt - usedmemlen;

    memset(self->used, 0, usedmemlen);

    ASAN_POISON_MEMORY_REGION(self->pool, self->poolsize);

    static_attest(continuous(*self));
}

/// get the number of bytes left in the pool
export fn free_bytes(Pool *self) -> usize
{
    static_attest(safe(self->used));
    static_attest(len(self->used) == self->poolsize/(usize)self->blocksize);

    usize mut c = 0;
    for (usize mut i = 0 ; i < self->poolsize/(usize)self->blocksize; i++) {

        static_attest(i/8 < len(self->used));
        if self->used[i/8] == 0xff {
            i += 7;
            continue;
        }

        static_attest(i/8 < len(self->used));
        if self->used[i/8] == 0x00 {
            c += 8 * (usize)self->blocksize;
            i += 7;
            continue;
        }

        static_attest(i/8 < len(self->used));
        if !bitarray_test(self->used, i) {
            c += (usize)self->blocksize;
        }
    }
    return c;
}

/// alloc a zeroed block
/// returns null if the pool is full
export fn alloc(Pool mut *self) -> void mut*
    where continuous(*self)
    model member(return, self)
    model continuous(*self)
{
    let r = self->malloc(self->blocksize - 8);

    static_attest(continuous(*self));

    return r;
}

/// malloc a zeroed continuous memory block of any size
/// returns null if the pool is full
/// this is alot less efficient than alloc()
/// as alignment requirements can lead to more blocks being allocated than you expect
///
export fn malloc(Pool mut *self, usize mut size) -> void mut*
    model member(return, self)
{
    static_attest(member(0, self));


    // 8 byte redzone
    size += 8;
    // 8 byte align
    if size % ALIGN > 0 {
        size += (ALIGN - (size % ALIGN));
    }


    usize mut blocks = size/(usize)self->blocksize;
    if size % (usize)self->blocksize != 0 {
        blocks += 1;
    }
    if blocks > 256 {
        return 0;
    }

    for (usize mut i = 0; i < self->poolsize/(usize)self->blocksize ; i++) {

        // optimization with faster byte compare
        static_attest(i/8 < len(self->used));
        if self->used[i/8] == 0xff {
            i+=7;
            continue;
        }


        static_attest(safe(self->used));
        static_attest(len(self->used) == self->poolsize/(usize)self->blocksize);

        usize mut i2 = i;
        bool mut allfree = true;
        for (usize mut j = 0; j < blocks; j++) {

            if i2 >= self->poolsize/(usize)self->blocksize {
                allfree = false;
                break;
            }
            if bitarray_test(self->used, i2) {
                allfree = false;
                break;
            }

            i2++;
        }

        if allfree {
            u8 mut * mut mem = 0;
            unsafe {
                mem = self->pool + ((usize)self->blocksize * i);
                ASAN_UNPOISON_MEMORY_REGION(mem, size);
                memset(mem, 0, size);
                mem[0] = 0x60;
                mem[1] = 0x61;
                mem[2] = 0x62;
                mem[3] = 0x63;
                mem[4] = blocks;
                mem[5] = 0x65;
                mem[6] = 0x66;
                mem[7] = 0x67;
                ASAN_POISON_MEMORY_REGION(mem, 8);
                mem = mem + 8;
            }

            for (usize mut j = 0; j < blocks; j++) {
                static_attest((i+j)/8 < len(self->used));
                bitarray_set(self->used, i+j);
            }

            err::assert((usize)mem % ALIGN == 0);
            return mem;
        }
    }
    return 0;
}

/// free a pointer previously allocated from this pool
/// requires pointer to be a member()
export fn free(Pool mut *self, void * unsafe mut ptr_)
    model continuous(*self)
    where member(ptr_, self)
{
    let mut ptr = (u8 mut*)ptr_;

    if ptr == 0 {
        static_attest(continuous(*self));
        return;
    }

    usize mut blocks;
    usize mut startblock;
    unsafe {
        ptr = ptr - 8;
        ASAN_UNPOISON_MEMORY_REGION(ptr, 8);
        blocks = ptr[4];
    }
    static_attest(len(ptr) >= 8);

    if ptr[0] != 0x60 || ptr[1] != 0x61 || ptr[2] != 0x62 || ptr[3] != 0x63 || ptr[7] != 0x67 {
        unsafe { hex::fdump(stderr, ptr, 8); }
        err::panic("invalid address passed to free");
    }

    ASAN_POISON_MEMORY_REGION(ptr, blocks * self->blocksize);

    err::assert((usize)blocks < self->poolsize/(usize)self->blocksize);
    unsafe {
        startblock = ((u8*)ptr - self->pool) / (usize)self->blocksize;
    }
    err::assert(startblock < self->poolsize/(usize)self->blocksize);

    for (usize mut i = startblock; i < startblock + (usize)blocks ; i++) {
        static_attest(safe(self->used));
        static_attest(i/8 < len(self->used));
        bitarray_clear(self->used, i);
    }
    static_attest(continuous(*self));
}

fn bitarray_set(u8 mut* a, usize index)
    where len(a) > index/8
{
    a[index/8] |= (u8)(1<<(index % 8));
}

fn bitarray_clear(u8 mut* a, usize index)
    where len(a) > index/8
{
    a[index/8] &= (u8)~(1<<(index % 8));
}

fn bitarray_test(u8 mut* a, usize index) -> bool
    where len(a) > index/8
{
    return (a[index/8] & (u8)(1<<(index % 8))) > 0;
}


export closure iterator(Pool mut*self, void mut *block, void mut * unsafe user);

/// run an iterator on every allocated block
/// this is useful when you want to use the pool as a vector
/// but cannot be used if you ever malloc()'d from this pool
///
/// it is safe to call free() and alloc() from iterator, because they are implemented as markers

export fn each(Pool mut*self, iterator it, void mut * unsafe user)
    where continuous(*self)
{
    static_attest(safe(self->used));
    static_attest(len(self->used) == self->poolsize/(usize)self->blocksize);

    for (usize mut i = 0; i < self->poolsize/(usize)self->blocksize ; i++) {
        static_attest(i/8 < len(self->used));
        if self->used[i/8] == 0x00 {
            i+=7;
            continue;
        }

        static_attest(i/8 < len(self->used));
        if bitarray_test(self->used, i) {
            unsafe{
                u8 mut* mut mem = self->pool + ((usize)self->blocksize * i);
                mem += 8;
                it.fn(self, mem, user, it.ctx);
            }
        }

    }
}

